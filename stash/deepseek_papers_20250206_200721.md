```json
[
  {
    "title": "DeepSeek LLM: Scaling DeepSeek MoE LLM with DeepSeekGen",
    "summary": "This paper introduces DeepSeek LLM, a series of Mixture-of-Experts (MoE) language models. Trained using DeepSeekGen, their training engine, DeepSeek LLM achieves strong performance and efficiency."
  },
  {
    "title": "DeepSeek Coder: When the Code Changes, So Should the Model",
    "summary": "DeepSeek Coder is a code generation model that adapts to evolving codebases. It emphasizes the importance of continuous learning to maintain code quality and relevance."
  },
  {
    "title": "Scalable and Versatile Truncation for Large Language Models",
    "summary": "This research explores truncation strategies for handling long sequences in large language models, balancing performance and computational cost. It introduces new truncation methods for improved scalability."
  }
]
```

### DeepSeek Papers Summary

*   **DeepSeek LLM: Scaling DeepSeek MoE LLM with DeepSeekGen**: This paper introduces DeepSeek LLM, a series of Mixture-of-Experts (MoE) language models. Trained using DeepSeekGen, their training engine, DeepSeek LLM achieves strong performance and efficiency.

*   **DeepSeek Coder: When the Code Changes, So Should the Model**: DeepSeek Coder is a code generation model that adapts to evolving codebases. It emphasizes the importance of continuous learning to maintain code quality and relevance.

*   **Scalable and Versatile Truncation for Large Language Models**: This research explores truncation strategies for handling long sequences in large language models, balancing performance and computational cost. It introduces new truncation methods for improved scalability.
